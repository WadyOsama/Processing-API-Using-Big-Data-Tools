# Big Data Engineering Project (Processing API Using Big Data Tools)

## Overview
This project demonstrates a data engineering pipeline designed on CentOS 6.5 with Hadoop and other Big Data tools. The pipeline pulls data from an external API, processes it in real-time, and stores it in a cloud database.

**Pipeline Summary:**
1. **Data Collection**: A Python script pulls user data from [RandomUser API](https://randomuser.me/api).
2. **Ingestion**: Apache Flume ingests the data and sends it to Apache Kafka.
4. **Storage & Processing**: Flume transfers a copy of the data from Kafka (as a consumer) to HDFS, while Spark processes data from Kafka (as a consumer) in real-time.
5. **Storage in Cloud**: Processed data is stored in InfluxDB on the cloud.
6. **Analysis & Visualization**:  Data is analyzed and visualized using Grafana Cloud, providing real-time insights and dashboards for monitoring user metrics and trends.

## Project Architecture
- **Python Script**: Fetches JSON data from RandomUser API and saves it to the local file system.
- **Apache Flume**: Captures JSON data from the file system, sends it to Kafka, and writes a copy to HDFS.
- **Apache Kafka**: Manages data streaming and messaging between producers and consumers.
- **Apache Spark**: Consumes data from Kafka for real-time processing.
- **InfluxDB**: Stores processed data for analytics and visualization on the cloud.
- **Grafana Cloud**:  Provides a platform for visualizing and analyzing data stored in InfluxDB, enabling the creation of interactive dashboards and reports to monitor user metrics and trends in real-time.

![Project Architecture Diagram](https://github.com/WadyOsama/Processing-API-Using-Big-Data-Tools/blob/main/Project_Diagram.gif)

---

## Prerequisites

- **Virtual Machine**: CentOS 6.5 (or any compatible machine depending on your setup)
- **Hadoop and HDFS**: For distributed storage
- **Apache Flume**: For data ingestion
- **Apache Kafka**: For data streaming
- **Apache Spark**: For real-time data processing
- **InfluxDB**: For cloud-based storage
- **Grafana Cloud**: For data visualization and analysis

Ensure all tools are properly installed and configured in your environment. Follow their official installation guides as necessary.

Adjust this project's files (Configurations & Python Scripts) to match your setup.

---

## Project Setup

### 1. Python Script for Data Collection
This script pulls user data from the RandomUser API and saves it to a specified local directory.

**`connection_script.py`**
```python
# connection_script.py
import requests
import time
import json
import os
from datetime import datetime

# Directory to save response files
output_dir = "/home/bigdata/api_logs"	#Modify this to match your desired directory
os.makedirs(output_dir, exist_ok=True)  # Create directory if it doesn't exist

while True:
    try:
	# Connect to the API to get a response
        response = requests.get("https://randomuser.me/api/?nat=us")
        if response.status_code == 200:
            fetched_data = response.json()

            # Create a unique filename based on the current timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = os.path.join(output_dir, f"response_{timestamp}.json")

            # Write the response to a separate file
            with open(filename, 'w') as json_file:
                json.dump(fetched_data, json_file, indent=None)  # Pretty print JSON

            print(f"Data logged to file: {filename}")
        else:
            print("Failed to fetch data from API")

    except Exception as e:
        print(f"An error occurred: {e}")

    time.sleep(1)  # Wait for 1 seconds before the next fetch
```
### 2. Configure Flume

Set up Flume with two configurations:
- The first configuration will pull data from the file generated by the Python script and send it to Kafka.
- The second configuration will consume data from Kafka and store it in HDFS.

#### Configuration 1: File Source to Kafka Sink

This configuration reads from `JSON files` (the file generated by the Python script) and sends the data to a Kafka topic.

**File to Kafka `api_to_kafka.conf`:**
```properties
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = spooldir
a1.sources.r1.channels = c1

#Modify this to match your directory
a1.sources.r1.spoolDir = /home/bigdata/api_logs

a1.sources.r1.fileHeader = true

a1.sinks.k1.channel = c1
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink

#Modify this to match your topic name
a1.sinks.k1.kafka.topic = user_logs

a1.sinks.k1.kafka.bootstrap.servers = localhost:9092
a1.sinks.k1.kafka.flumeBatchSize = 20
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.kafka.producer.linger.ms = 1
a1.sinks.k1.kafka.producer.compression.type = snappy

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 1000

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```
#### Configuration 2: Kafka Source to HDFS Sink

This configuration reads data from the Kafka topic and writes it to HDFS.

**Kafka to HDFS `kafka_to_hdfs.conf`:**
```properties
# Name the components on this agent
a2.sources = r2
a2.sinks = k2
a2.channels = c2

# Describe/configure the source
a2.sources.r2.type = org.apache.flume.source.kafka.KafkaSource
a2.sources.r2.channels = c2
a2.sources.r2.batchSize = 5000
a2.sources.r2.batchDurationMillis = 2000
a2.sources.r2.kafka.bootstrap.servers = localhost:9092

#Modify this to match your topic name
a2.sources.r2.kafka.topics = user_logs

# Describe the sink
a2.sinks.k2.type = hdfs
a2.sinks.k2.channel = c2

#Modify this to match your desired HDFS directory
a2.sinks.k2.hdfs.path = /final_project

a2.sinks.k2.hdfs.filePrefix = events-
a2.sinks.k2.hdfs.round = true
a2.sinks.k2.hdfs.roundValue = 10
a2.sinks.k2.hdfs.roundUnit = minute

# Use a channel which buffers events in memory
a2.channels.c2.type = memory
a2.channels.c2.capacity = 5000
a2.channels.c2.transactionCapacity = 5000

# Bind the source and sink to the channel
a2.sources.r2.channels = c2
a2.sinks.k2.channel = c2
```
### 3. Configure Kafka
Set up Kafka to receive data from Flume.

- Create a new topic:
  ```bash
  kafka-topics.sh --create --topic <my-kafka-topic> --bootstrap-server localhost:9092
  ```
  **The project's topic name is `user_logs`**

  ```bash
  kafka-topics.sh --create --topic user_logs --bootstrap-server localhost:9092
  ```
### 4. Configure Spark
Use Spark to consume data from Kafka, process it, and send results to InfluxDB.

#### Spark Script
This Spark application reads data from Kafka, processes it, and writes it to InfluxDB.

***Make sure to replace placeholders such as `your-influxdb-url`, `your-org-id`, `your-bucket-name`, `precision`, `your-influxdb-token`***

**Spark Application `pyspark_influx.py`:**
```python
import kafka
import requests
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, explode
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType
from io import StringIO

# Initialize Spark session with Kafka support
spark = SparkSession.builder \
    .appName("Kafka-PySpark-Integration") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1") \
    .getOrCreate()

# InfluxDB Configuration
influxdb_url = "https://<your-influxdb-url>/api/v2/write?org=<your-org-id>&bucket=<your-bucket-name>&precision=<precision>"
influxdb_token = "<your-influxdb-token>"

# Define Kafka Variables
kafka_bootstrap_servers = "localhost:9092"
kafka_topic = "user_logs"

# Initialize a set to track processed records
processed_records = set()

# Structure for JSON files
schema = StructType([
    StructField("results", ArrayType(
        StructType([
            StructField("gender", StringType(), True),
            StructField("name", StructType([
                StructField("title", StringType(), True),
                StructField("first", StringType(), True),
                StructField("last", StringType(), True)
            ]), True),
            StructField("location", StructType([
                StructField("street", StructType([
                    StructField("number", IntegerType(), True),
                    StructField("name", StringType(), True)
                ]), True),
                StructField("city", StringType(), True),
                StructField("state", StringType(), True),
                StructField("country", StringType(), True),
                StructField("postcode", IntegerType(), True),
                StructField("coordinates", StructType([
                    StructField("latitude", StringType(), True),
                    StructField("longitude", StringType(), True)
                ]), True),
                StructField("timezone", StructType([
                    StructField("offset", StringType(), True),
                    StructField("description", StringType(), True)
                ]), True)
            ]), True),
            StructField("email", StringType(), True),
            StructField("login", StructType([
                StructField("uuid", StringType(), True),
                StructField("username", StringType(), True),
                StructField("password", StringType(), True),
                StructField("salt", StringType(), True),
                StructField("md5", StringType(), True),
                StructField("sha1", StringType(), True),
                StructField("sha256", StringType(), True)
            ]), True),
            StructField("dob", StructType([
                StructField("date", StringType(), True),
                StructField("age", IntegerType(), True)
            ]), True),
            StructField("registered", StructType([
                StructField("date", StringType(), True),
                StructField("age", IntegerType(), True)
            ]), True),
            StructField("phone", StringType(), True),
            StructField("cell", StringType(), True),
            StructField("id", StructType([
                StructField("name", StringType(), True),
                StructField("value", StringType(), True)
            ]), True),
            StructField("picture", StructType([
                StructField("large", StringType(), True),
                StructField("medium", StringType(), True),
                StructField("thumbnail", StringType(), True)
            ]), True),
            StructField("nat", StringType(), True)
        ])
    ), True),
    StructField("info", StructType([
        StructField("seed", StringType(), True),
        StructField("results", IntegerType(), True),
        StructField("page", IntegerType(), True),
        StructField("version", StringType(), True)
    ]), True)
])

# Read data from Kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
    .option("subscribe", kafka_topic) \
    .load()

# Casting the binary data to string
value_df = df.selectExpr("CAST(value AS STRING)")

# Make it structured with the defined schema
structured_df = value_df.select(from_json(col("value"), schema).alias("data")).select("data.*")

# Flatten the file
flattened_df = structured_df.select(
    col("results")
).withColumn("result", explode(col("results")))  # Flatten the results array

# Extract individual fields from the exploded result
final_df = flattened_df.select(
    col("result.name.title"),
    col("result.name.first"),
    col("result.name.last"),
    col("result.gender"),
    col("result.email"),
    col("result.location.street"),
    col("result.location.city"),
    col("result.location.state"),
    col("result.location.country"),
    col("result.location.postcode"),
    col("result.location.coordinates.latitude"),
    col("result.location.coordinates.longitude"),
    col("result.dob.age"),
    col("result.phone")
)

# Function sending the data to InfluxDB
def write_to_influxdb(data):
    
    # Use email as a unique identifier
    unique_id = data.get('email')  
    if unique_id in processed_records:
        return  # Skip if already processed
    
    # After successfully writing, add the unique ID to the processed set
    processed_records.add(unique_id)
    
    # Measurement name
    measurement = "users"

    # Prepare tag sets
    tags = []
    if data.get('title'):
        tags.append(f"title={data['title']}")
    if data.get('gender'):
        tags.append(f"gender={data['gender']}")
    if data.get('email'):
        tags.append(f"email={data['email']}")
    
    tags_str = ','.join(tags)

    # Prepare field sets
    fields = []
    if data.get('first'):
        fields.append(f"first=\"{data['first']}\"")
    if data.get('last'):
        fields.append(f"last=\"{data['last']}\"")
    if data.get('street'):
        fields.append(f"street=\"{data['street']}\"")
    if data.get('city'):
        fields.append(f"city=\"{data['city']}\"")
    if data.get('state'):
        fields.append(f"state=\"{data['state']}\"")
    if data.get('country'):
        fields.append(f"country=\"{data['country']}\"")
    if data.get('postcode'):
        fields.append(f"postcode=\"{data['postcode']}\"")
    if data.get('latitude'):
        fields.append(f"latitude=\"{data['latitude']}\"")
    if data.get('longitude'):
        fields.append(f"longitude=\"{data['longitude']}\"")
    if data.get('age') is not None:  # Check for None instead of empty string
        fields.append(f"age={data['age']}")
    if data.get('phone'):
        fields.append(f"phone=\"{data['phone']}\"")

    fields_str = ','.join(fields)

    # Combine into line protocol format
    line_protocol_data = f"{measurement},{tags_str} {fields_str}"

    # Prepare headers
    headers = {
        "Authorization": f"Token {influxdb_token}",
        "Content-Type": "text/plain; charset=utf-8",
    }

    # Send the line protocol data to InfluxDB
    response = requests.post(influxdb_url, data=line_protocol_data, headers=headers)
    
    # Tracking the request result
    if response.status_code != 204:
        print(f"Error writing to InfluxDB: {response.text}")
    else:
        print('Done!')

# Function to process every batch of data
def process_batch(batch_df, batch_id):
    for row in batch_df.collect():
        # Convert row to dictionary
        data = row.asDict()
        # Write each record to InfluxDB
        write_to_influxdb(data)

# Process the streaming data
query = final_df.writeStream \
    .outputMode("append") \
    .foreachBatch(process_batch) \
    .start()

# Await termination
query.awaitTermination()

# Stop the Spark session
spark.stop()
```
### 5. Configure InfluxDB Cloud
Set up InfluxDB Cloud to store processed data from Spark. 
Follow the [InfluxDB installation guide](https://docs.influxdata.com/influxdb/cloud/get-started/setup/) to set up InfluxDB Cloud.

1. **Sign Up and Create a Bucket**: 
   - If you haven't already, sign up for an InfluxDB Cloud account at [InfluxDB Cloud](https://cloud2.influxdata.com/signup).
   - Once logged in, create a new bucket where you will store the processed data. 

2. **Retrieve Connection Details**: 
   - Go to your InfluxDB Cloud dashboard and navigate to the **Data** section to find your organization and bucket information.
   - Collect your **API Token**, **organization ID**, and **bucket name**, as you will need these for your Spark application.

3. **Configure Connection in Spark**: 
   Ensure your Spark application is configured to connect to InfluxDB Cloud by replacing the placeholders in your Spark script with your InfluxDB Cloud details:
   - `your-influxdb-url`: Use the InfluxDB Cloud API endpoint (e.g., `https://us-west-2-1.aws.cloud2.influxdata.com`).
   - `your_bucket-name`: The name of the bucket you created.
   - `your_org-id`: Your organization ID from InfluxDB Cloud.
   - `your_api_token`: The API token you generated.

### 6. Configure Grafana Cloud

1. **Sign Up and Create a Dashboard**: 
   - Sign up at [Grafana Cloud](https://grafana.com/products/cloud/) and create a new dashboard for visualizing data.

2. **Add InfluxDB Data Source**: 
   - In the dashboard, navigate to **Data Sources** and select **Add Data Source**. Choose **InfluxDB**.

3. **Configure InfluxDB Connection**: 
   - Fill in the connection details:
     - **URL**: Use your InfluxDB Cloud API endpoint.
     - **Database**: Enter your bucket name.
     - **Password**: Input your API token.
  
4. **Test Data Source**: 
   - Click **Save & Test** to ensure the connection is successful.

5. **Create Visualizations**: 
   - Add panels to your dashboard for various visualizations like time series graphs and tables, customizing queries and settings as needed.

6. **Set Up Alerts**: 
   - Optionally, configure alerts to notify you of significant data changes by enabling alerts in the panel settings and defining notification conditions.

---

## Usage

1. **Ensure all of the Prerequisites services working (HDFS, YARN, ZooKeeper, Kafka)**:
   - For HDFS & YARN:
     ```bash
     start-all.sh
     ```
   - For ZooKeeper & Kafka"
     ```bash
     cd $KAFKA_HOME
     ```
     ```bash
     bin/zookeeper-server-start.sh config/zookeeper.properties
     ```
     ```bash
     bin/kafka-server-start.sh config/server.properties
     ```
2. **Create Kafka Topic**:
     ```bash
     cd $KAFKA_HOME
     ```
     ```bash
     bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic user_logs
     ```
3. **Put Flume config files in config directory**:
     ```bash
     cp /<your-path>/api_to_kafka.conf $FLUME_HOME/conf
     ```
     ```bash
     cp /<your-path>/kafka_to_hdfs.conf $FLUME_HOME/conf
     ```
4. **Run Both of Flume Agents**:
   ```bash
   $FLUME_HOME/bin/flume-ng agent --conf conf --conf-file $FLUME_HOME/conf/api_to_kafka.conf --name a1 -Dflume.root.logger=DEBUG,console
   ```
   ```bash
   $FLUME_HOME/bin/flume-ng agent --conf conf --conf-file $FLUME_HOME/conf/kafka_to_hdfs.conf --name a2 -Dflume.root.logger=DEBUG,console
   ```
5. **Run the Python script to fetch data**:
   ```bash
   python3 /<your-path>/connection_script.py
   ```
6. **Submit the Spark job**:
   
   	**Note that you must have the needed packages to connect to Kafka from Spark.**
   
   	**You also must change the numbers in this command to match your versions of Kafka and Sprak**
   ```bash
   spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1 pyspark_influx.py 
   ```
6. **Verify data in InfluxDB Cloud**:
   
   Check your InfluxDB Cloud dashboard to ensure data is being written to your specified bucket.
   
![InfluxDB Results](https://github.com/WadyOsama/Processing-API-Using-Big-Data-Tools/blob/main/My-Results/InfluxDB.jpeg)

---
## Data Analysis & Visualization with Grafana

To visualize and analyze the data ingested into InfluxDB Cloud, I utilized **Grafana**, a powerful open-source analytics and monitoring platform. Grafana allows for real-time monitoring and data visualization, making it an essential tool in my project. Below are key points about how I integrated Grafana into my project:

- **Data Visualization**: Grafana enables the creation of dynamic dashboards, where I can visualize time-series data collected from various sources. This helped in identifying trends, patterns, and anomalies within the data.

- **Integration with InfluxDB Cloud**: I configured Grafana to connect to my InfluxDB Cloud instance, allowing for seamless querying of data stored in the InfluxDB database. This integration facilitated real-time data analysis through Flux queries.

- **Custom Dashboards**: I created custom dashboards tailored to specific metrics relevant to my project. These dashboards include various visualizations such as graphs, tables, and gauges, providing a comprehensive view of the data.

- **Alerts and Notifications**: Grafana's alerting features enabled me to set up notifications based on certain thresholds, ensuring timely responses to significant changes in the data.

By leveraging Grafana, I was able to enhance the analytical capabilities of my project, making the data not only accessible but also actionable.

![Grafana Results](https://github.com/WadyOsama/Processing-API-Using-Big-Data-Tools/blob/main/My-Results/Grafana.jpeg)


---

## Project Status and Future Work

This project successfully establishes a real-time data pipeline using Flume, Kafka, Spark, and InfluxDB Cloud. The data flows seamlessly from data generation through processing and storage in the cloud.

### Future Enhancements

- **Data Processing Optimization**: Implement advanced data transformation techniques in Spark, such as structured streaming optimizations and partitioning strategies, to handle even larger datasets more efficiently.

- **Expanded Data Sources and Outputs**: 
   - Add new data sources or APIs to enrich the dataset.
   - Consider integrating additional output storage or analytics solutions, such as Elasticsearch, for more versatile data use cases.

---

## Acknowledgments

- [RandomUser API](https://randomuser.me/) for providing the data generation API.
- [Apache Flume](https://flume.apache.org/) for its data ingestion capabilities.
- [Apache Kafka](https://kafka.apache.org/) for managing data streaming.
- [Apache Spark](https://spark.apache.org/) for real-time data processing.
- [Apache Hadoop](https://hadoop.apache.org/) for distributed storage solutions.
- [InfluxDB Cloud](https://www.influxdata.com/products/influxdb-cloud/) for its powerful cloud-based data storage and processing tools.
- [Grafana](https://grafana.com/) for its exceptional data visualization and monitoring capabilities.

